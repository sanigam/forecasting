{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Load specific forecasting tools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf # for determining (p,q) orders\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose      # for ETS Plots\n",
    "from pmdarima import auto_arima                              # for determining ARIMA orders\n",
    "\n",
    "# Load specific evaluation tools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from fbprophet import Prophet\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation(input_dim, h1, h2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(h1, input_dim=input_dim))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(h2))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return (model)\n",
    "\n",
    "#convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1, offset=0):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-offset):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back+offset])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def nn_pred(train_nn):\n",
    "    model = model_creation(12,6,6)\n",
    "    nn_train = train_nn['y'].values\n",
    "    X, y = create_dataset(nn_train, 12)\n",
    "    model.fit(X, y, epochs=20,shuffle=True,verbose=0)\n",
    "    predictions_nn=[]\n",
    "    trainy = train['y'].values\n",
    "    for i in range(3):\n",
    "        \n",
    "        nn_train = trainy[-12:].reshape(-1,12)\n",
    "        pred = model.predict(nn_train)\n",
    "        predictions_nn.append(pred.flatten()[0])\n",
    "        trainy = np.append(trainy, round(pred.flatten()[0]) )\n",
    "        \n",
    "    predictions_nn = np.array(predictions_nn).round()\n",
    "    return(predictions_nn )\n",
    "\n",
    "def lstm_pred(train, testlen):\n",
    "    n_input = 12\n",
    "    n_features=1\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    scaled_train = scaler.transform(train)\n",
    "    #scaled_test = scaler.transform(test)\n",
    "\n",
    "    generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit_generator(generator,epochs=25, verbose = 0)\n",
    "\n",
    "    test_predictions = []\n",
    "    first_eval_batch = scaled_train[-n_input:]\n",
    "    current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "    #print(current_batch)\n",
    "    for i in range(testlen):\n",
    "        #print(current_batch[0])\n",
    "        current_pred = model.predict(current_batch)[0]\n",
    "        test_predictions.append(current_pred) \n",
    "        # From 2nd dim of current batch, drop first value and add latest predition\n",
    "        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "    true_predictions = np.round(scaler.inverse_transform(test_predictions).flatten())\n",
    "    return true_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('.xlsx', sheet_name = 'PLID Bookings')\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc = pd.read_excel('.xlsx', sheet_name = 'Group 1 - Your Forecast')\n",
    "print(df_fc.shape)\n",
    "print(df_fc['PLID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc['model_num'] =  None\n",
    "df_fc['rmse'] = None\n",
    "df_fc['rmse_pct'] = None\n",
    "df_fc['all_mean'] = None\n",
    "df_fc['test_mean'] = None\n",
    "df_fc['train_mean']= None\n",
    "df_fc['latest_6']= None\n",
    "df_fc['predicted']= None\n",
    "df_fc['accuracy']= None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ds'] = pd.to_datetime(df['Fiscal Year Month Number'], format=\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting plids with missing months\n",
    "for idx, plid in enumerate(df_fc['PLID'].values):   \n",
    "    df1 = df [ (df['PLID'] ==  plid ) ]\n",
    "    #print(f'\\n*****{idx} - PLID: {plid} Shape: {df1.shape}*****')\n",
    "    ds_array = np.arange ( df1['ds'].min(), df1['ds'].max()+ \n",
    "                      relativedelta(months=1), 1 , dtype='datetime64[M]') \n",
    "    #print(ds_array)\n",
    "    df1.sort_values(by = 'ds', inplace=True)\n",
    "    if len(ds_array) == len(df1): \n",
    "        df1['ds'] = ds_array\n",
    "    else:\n",
    "        #print (f\"'{plid}',\")\n",
    "        print (plid,len(ds_array),len(df1)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df [ (df['PLID'] ==  'C9300-48UXM' ) ]\n",
    "print(df1.shape)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=['XXXXX']\n",
    "for plid in p:\n",
    "    ds_array = np.arange ( df1['ds'].min(), df1['ds'].max()+ \n",
    "                      relativedelta(months=1), 1 , dtype='datetime64[M]') \n",
    "    for dt in ds_array:\n",
    "        if len( df1[df1['ds']==dt]) != 0:\n",
    "            y = df1[df1['ds']==dt]['MFG Rev Booked Units'].values[0]\n",
    "            print (y, int(y))    \n",
    "        else:\n",
    "            df1 = df1.append({'PLID': plid, 'ds':dt,'MFG Rev Booked Units': y }, ignore_index=True)\n",
    "print(df1.shape)\n",
    "df1.sort_values(by = 'ds', inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for idx, plid in enumerate(df_fc['PLID'].values[77:]) :\n",
    "    df1 = df [ (df['PLID'] ==  plid ) ]\n",
    "    print(f'\\n*****{idx} - PLID: {plid} Shape: {df1.shape} {time.asctime()}*****')\n",
    "    \n",
    "    #If rows are less than 4 just put average                       \n",
    "    if df1.shape[0] <= 6:\n",
    "        q_forecast = round(df1['MFG Rev Booked Units'].mean()*3/df1.shape[0])\n",
    "        model_num=99     \n",
    "        cols = ['Q1 FY2021 Forecast (Units)', 'model_num']              \n",
    "        df_fc.loc[df_fc['PLID']==plid, cols] = [q_forecast, model_num] \n",
    "        continue\n",
    "    \n",
    "    df1.sort_values(by = 'ds', inplace=True)\n",
    "\n",
    "    ds_array = np.arange ( df1['ds'].min(), df1['ds'].max()+ \n",
    "                      relativedelta(months=1), 1 , dtype='datetime64[M]') \n",
    "    \n",
    "    #Extraploating missing values in case dates are missing\n",
    "    if len(ds_array) != len(df1):\n",
    "        for dt in ds_array:\n",
    "            if len( df1[df1['ds']==dt]) != 0:\n",
    "                y = df1[df1['ds']==dt]['MFG Rev Booked Units'].values[0]\n",
    "               \n",
    "            else:\n",
    "                df1 = df1.append({'PLID': plid, 'ds':dt,'MFG Rev Booked Units': y }, ignore_index=True)\n",
    "            df1.sort_values(by = 'ds', inplace=True)\n",
    "    \n",
    "    df1 = df1[['ds', 'MFG Rev Booked Units'] ]\n",
    "    df1.columns = ['ds', 'y']\n",
    " \n",
    "    #df1['ds'] = pd.to_datetime(df1['ds'])\n",
    "    train = df1.iloc[:-3]\n",
    "    test = df1.iloc[-3:]\n",
    "    test_mean = round (test['y'].mean())\n",
    "    train_mean = round (train['y'].mean())\n",
    "    all_mean = round (df1['y'].mean())\n",
    "    print(f'All Mean: {all_mean} Train Mean: {train_mean} Test Mean: {test_mean}')\n",
    "    #print(train)\n",
    "    \n",
    "#   Bias = (Forecast-Actual)/Actual\n",
    "#   Accuracy = 1 - Abs(Bias)\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Prohet model\n",
    "    try:\n",
    "        m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality= False)\n",
    "        m.fit(train)\n",
    "        future = m.make_future_dataframe(len(test), freq='MS')\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        predictions = forecast.iloc[-1*len(test):]['yhat']\n",
    "        error_p_a = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_p_a = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Prophet Exception')\n",
    "        error_p_a = math.nan\n",
    "        acc_p_a = math.nan\n",
    "        \n",
    "    \n",
    "    # Prohet model Multiplicative\n",
    "    try:\n",
    "        m = Prophet(yearly_seasonality=True , weekly_seasonality=False, daily_seasonality= False, \n",
    "                    seasonality_mode='multiplicative')\n",
    "        m.fit(train)\n",
    "        future = m.make_future_dataframe(len(test), freq='MS')\n",
    "\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        predictions = forecast.iloc[-1*len(test):]['yhat']\n",
    "        error_p_m = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_p_m = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Prophet Exception')\n",
    "        error_p_m = math.nan\n",
    "        acc_p_m = math.nan\n",
    "    \n",
    "    print(f'Prophet Error - Add:{error_p_a} Mul:{error_p_m}, Prophet Accuracy - Add:{acc_p_a} Mul:{acc_p_m} ' ) \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "    #ARIMA\n",
    "    df1.set_index('ds', inplace=True)\n",
    "    df1.index.freq = 'MS'\n",
    "    train = df1.iloc[:-3]\n",
    "    test = df1.iloc[-3:]\n",
    "  \n",
    "\n",
    "    \n",
    "    if len(df1)> 39:\n",
    "        s=True\n",
    "        m=12\n",
    "    else:\n",
    "        s = False\n",
    "        m=1\n",
    "    \n",
    "   \n",
    "    #print(df1.info(), df1.index, df1.shape, df1)\n",
    "    try:\n",
    "        model = auto_arima(df1['y'], seasonal=s, m=m, trend='c')\n",
    "        model = SARIMAX( train['y'], order = model.order, seasonal_order= model.seasonal_order)\n",
    "        results = model.fit()\n",
    "        predictions = results.predict ( start = len(train), end=len(train)+len(test)-1, dynamic=False, \n",
    "                                 typ='levels').rename('Prediction')\n",
    "        error_a_c1 = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_a_c1 = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception')\n",
    "        error_a_c1 = math.nan\n",
    "        acc_a_c1 = math.nan\n",
    "        \n",
    "  \n",
    "    try:\n",
    "        model = auto_arima(df1['y'], seasonal=s, m=m, trend='t')\n",
    "        model = SARIMAX( train['y'], order = model.order, seasonal_order= model.seasonal_order)\n",
    "        results = model.fit()\n",
    "        predictions = results.predict ( start = len(train), end=len(train)+len(test)-1, dynamic=False, \n",
    "                                       typ='levels').rename('Prediction')\n",
    "        error_a_t1 = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_a_t1 = round( 1- abs(bias ),4)\n",
    "        \n",
    "    except:\n",
    "        print ('Exception')\n",
    "        error_a_t1 = math.nan\n",
    "        acc_a_t1 = math.nan\n",
    "   \n",
    "    try:\n",
    "        model = auto_arima(df1['y'], seasonal=True, m=m, trend='ct')\n",
    "        model = SARIMAX( train['y'], order = model.order, seasonal_order= model.seasonal_order)\n",
    "        results = model.fit()\n",
    "        predictions = results.predict ( start = len(train), end=len(train)+len(test)-1, dynamic=False, \n",
    "                                       typ='levels').rename('Prediction')\n",
    "        error_a_ct1 = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_a_ct1 = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception')\n",
    "        error_a_ct1 = math.nan\n",
    "        acc_a_ct1 = math.nan\n",
    "    \n",
    "    print(f'ARIMA Error - c:{error_a_c1} t:{error_a_t1}  ct:{error_a_ct1}, ARIMA Accuracy - c:{acc_a_c1} t:{acc_a_t1}  ct:{acc_a_ct1}  ' ) \n",
    "  \n",
    "    \n",
    "    #Holt winter\n",
    "    if len(df1)>= 27:\n",
    "        m1=12\n",
    "    else:\n",
    "        m1=3\n",
    "    \n",
    "    try:\n",
    "        fitted_model = ExponentialSmoothing(train['y'],trend='add',seasonal='add',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(len(test)).rename('HW Forecast')\n",
    "        error_hwaa = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_hwaa = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception HWaa')\n",
    "        error_hwaa = math.nan\n",
    "        acc_hwaa = math.nan\n",
    "    \n",
    "    try:\n",
    "        fitted_model = ExponentialSmoothing(train['y'],trend='add',seasonal='mul',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(len(test)).rename('HW Forecast')\n",
    "        error_hwam = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_hwam = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception in HWam')\n",
    "        error_hwam = math.nan\n",
    "        acc_hwam = math.nan\n",
    "    \n",
    "    try:\n",
    "        fitted_model = ExponentialSmoothing(train['y'],trend='mul',seasonal='add',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(len(test)).rename('HW Forecast')\n",
    "        error_hwma = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_hwma = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception in HWma')\n",
    "        error_hwma = math.nan\n",
    "        acc_hwma = math.nan\n",
    "    \n",
    "    try:\n",
    "        fitted_model = ExponentialSmoothing(train['y'],trend='mul',seasonal='mul',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(len(test)).rename('HW Forecast')\n",
    "        error_hwmm = round(rmse(predictions,test['y']),2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_hwmm = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception HWmm')\n",
    "        error_hwmm = math.nan\n",
    "        acc_hwmm = math.nan\n",
    "        \n",
    "    print(f'HW Error - aa:{error_hwaa} am:{error_hwam} ma:{error_hwma} mm:{error_hwmm}, HW Accuracy - aa:{acc_hwaa} am:{acc_hwam} ma:{acc_hwma} mm:{acc_hwmm}  ' ) \n",
    "   \n",
    "    #Neural Network\n",
    "    try:\n",
    "        predictions = lstm_pred(train, len(test) )\n",
    "        error_nn = round( np.sqrt(mean_squared_error(predictions,test['y'].values)), 2)\n",
    "        bias = (predictions.sum() - test['y'].values.sum())/test['y'].values.sum()\n",
    "        acc_nn = round( 1- abs(bias ),4)\n",
    "    except:\n",
    "        print ('Exception in lstm model')\n",
    "        error_nn = math.nan\n",
    "        acc_nn = math.nan\n",
    "    print(f'LSTM Error: {error_nn}, LSTM Accuracy: {acc_nn}')\n",
    "    \n",
    "#     error_nn = math.nan\n",
    "#     acc_nn = math.nan\n",
    "    \n",
    "    error_list = [error_p_a, error_p_m,error_a_c1, error_a_t1, error_a_ct1 , error_hwaa, error_hwam, \n",
    "                  error_hwma, error_hwmm, error_nn]\n",
    "    error_list = np.nan_to_num(error_list, copy=True, nan=math.inf)\n",
    "    \n",
    "    acc_list = [acc_p_a, acc_p_m,acc_a_c1, acc_a_t1, acc_a_ct1 , acc_hwaa, acc_hwam, \n",
    "                  acc_hwma, acc_hwmm, acc_nn]\n",
    "    acc_list = np.nan_to_num(acc_list, copy=True, nan= -math.inf)\n",
    "    \n",
    "    #model_num = np.argmin(error_list)\n",
    "    model_num = np.argmax(acc_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(model_num)\n",
    "\n",
    "    rmse_final = error_list[model_num]\n",
    "    rmse_pct = round(error_list[model_num]*100/test_mean,2)\n",
    "    acc_final = acc_list[model_num]\n",
    "    latest_6 = df1['y'].values[-6:]\n",
    "    if model_num == 0:\n",
    "        if df1.shape[1] == 1:\n",
    "            df1.reset_index(inplace=True)\n",
    "        m = Prophet()\n",
    "        m.fit(df1)\n",
    "        future = m.make_future_dataframe(3, freq='MS')\n",
    "        forecast = m.predict(future)\n",
    "        predictions = forecast.iloc[-3:]['yhat'].values\n",
    "    elif model_num == 1:\n",
    "        if df1.shape[1] == 1:\n",
    "            df1.reset_index(inplace=True)\n",
    "        m = Prophet(seasonality_mode='multiplicative')\n",
    "        m.fit(df1)\n",
    "        future = m.make_future_dataframe(3, freq='MS')\n",
    "        forecast = m.predict(future)\n",
    "        predictions = forecast.iloc[-3:]['yhat'].values\n",
    "    elif model_num == 2:\n",
    "        model = auto_arima(df1['y'], seasonal=s, m=m, trend='c')\n",
    "        model = SARIMAX( df1['y'], order = model.order, seasonal_order= model.seasonal_order)\n",
    "        results = model.fit()\n",
    "        predictions = results.predict ( start = len(df1), end=len(df1)+2, dynamic=False, \n",
    "                                   typ='levels').values\n",
    "    elif model_num == 3:\n",
    "        model = auto_arima(df1['y'], seasonal=s, m=m, trend='t')\n",
    "        model = SARIMAX( df1['y'], order = model.order, seasonal_order= model.seasonal_order)\n",
    "        results = model.fit()\n",
    "        predictions = results.predict ( start = len(df1), end=len(df1)+2, dynamic=False, \n",
    "                                   typ='levels').values\n",
    "    elif model_num == 4:\n",
    "        model = auto_arima(df1['y'], seasonal=s, m=m, trend='ct')\n",
    "        model = SARIMAX( df1['y'], order = model.order, seasonal_order= model.seasonal_order)\n",
    "        results = model.fit()\n",
    "        predictions = results.predict ( start = len(df1), end=len(df1)+2, dynamic=False, \n",
    "                                   typ='levels').values\n",
    "    elif model_num == 5:\n",
    "        fitted_model = ExponentialSmoothing(df1['y'],trend='add',seasonal='add',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(3).values\n",
    "    elif model_num == 6:\n",
    "        fitted_model = ExponentialSmoothing(df1['y'],trend='add',seasonal='mul',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(3).values\n",
    "    elif model_num == 7:\n",
    "        fitted_model = ExponentialSmoothing(df1['y'],trend='mul',seasonal='add',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(3).values\n",
    "    elif model_num == 8:\n",
    "        fitted_model = ExponentialSmoothing(df1['y'],trend='mul',seasonal='mul',seasonal_periods=m1).fit()\n",
    "        predictions = fitted_model.forecast(3).values\n",
    "    elif model_num == 9:\n",
    "        predictions = lstm_pred(df1,3)\n",
    "    else:\n",
    "        print('There is an issue')\n",
    "    \n",
    "    print (f'Model Num:{model_num} RMSE={rmse_final} RMSE%={rmse_pct} Accuracy={acc_final} Latest 6={latest_6} Predictions={predictions.round()}')\n",
    "    \n",
    "#     plt.figure(figsize=(10,1))\n",
    "#     plt.plot(range(9), np.concatenate([latest_6,predictions]), marker='o' )\n",
    "    \n",
    "    q_forecast = round(predictions.sum())\n",
    "    #print(plid, q_forecast, rmse_final,rmse_pct, test_mean,train_mean, model_num)\n",
    "    \n",
    "    cols = ['Q1 FY2021 Forecast (Units)', 'model_num', 'rmse', 'rmse_pct', 'accuracy','all_mean','test_mean', \\\n",
    "            'train_mean', 'latest_6', 'predicted' ]\n",
    "    \n",
    "    df_fc.loc[df_fc['PLID']==plid, cols] = \\\n",
    "         [q_forecast, model_num, rmse_final,rmse_pct,acc_final, all_mean, test_mean,train_mean, str(latest_6),str(predictions.round()) ] \n",
    "    \n",
    "#     print(df1.shape, predictions.shape)   \n",
    "#     title='Booking Forecast Model #' + str(model_num) \n",
    "#     ylabel='Monthly Bookings'\n",
    "#     xlabel=''\n",
    "#     ax = df1.plot(legend=True,figsize=(12,4),title=title)\n",
    "#     predictions.plot(legend=True)\n",
    "#     ax.autoscale(axis='x',tight=True)\n",
    "#     ax.set(xlabel=xlabel, ylabel=ylabel);\n",
    "#     time.sleep(0.1)\n",
    "#     plt.pause(0.0001)  \n",
    "        \n",
    "    \n",
    "\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc[df_fc.PLID=='NCS1K4-1.2TL-K9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc.to_csv('forecast_q1_acc_v1_part2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc = pd.read_csv('forecast_q1_acc_v1_part2.csv')\n",
    "df_fc.groupby('model_num').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc[df_fc['model_num'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc[df_fc['PLID']=='A9K-RSP880']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,1))\n",
    "plt.plot(range(9), np.array([4,5,7,8,9,0,7,8,9]), marker='o' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([5,2])\n",
    "np.concatenate([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a, b, linestyle='solid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel( 'Fantasy Forecast data_V2 Q4 data set.xlsx', sheet_name = 'Forecast streams', header= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('forecast_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('forecast_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_1 = pd.merge( df1, df , on='PLID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_1.to_csv ('forecast_3_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.read_excel( 'Fantasy Forecast data_V2 Q4 data set.xlsx', sheet_name = 'Qtr metrics', header= 0)\n",
    "df_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = df_metrics[df_metrics['QTR'] == 'Q3 FY2020']\n",
    "df_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_1 = pd.merge( df1, df_metrics , on='PLID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_1.to_csv ('forecast_3_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
